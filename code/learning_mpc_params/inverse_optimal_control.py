# í•µì‹¬ IOC ì•Œê³ ë¦¬ì¦˜

"""
Inverse Optimal Control using Relaxed KKT Conditions

ì „ë¬¸ê°€ ì‹œì—° ë°ì´í„°ë¡œë¶€í„° MPC ë¹„ìš©í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—­ìœ¼ë¡œ ì¶”ì •í•©ë‹ˆë‹¤.
KKT ì¡°ê±´ ì™„í™”(Relaxed KKT) ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

í•µì‹¬ ì•„ì´ë””ì–´:
    ìµœì ì„± ì¡°ê±´ âˆ‡L = 0ì„ ì •í™•íˆ ë§Œì¡±ì‹œí‚¤ëŠ” ëŒ€ì‹ ,
    â€–âˆ‡Lâ€–Â²ì„ ìµœì†Œí™”í•˜ëŠ” íŒŒë¼ë¯¸í„° Î¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
"""

import numpy as np
import mujoco
from scipy.optimize import minimize, LinearConstraint
import matplotlib.pyplot as plt
from typing import Dict, Tuple, List


class InverseOptimalControl:
    """
    KKT ì¡°ê±´ ì™„í™”ë¥¼ ì´ìš©í•œ ì—­ìµœì ì œì–´
    
    ì „ë¬¸ê°€ ì‹œì—°ìœ¼ë¡œë¶€í„° MPC ë¹„ìš©í•¨ìˆ˜ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, model, joint_ids, horizon, dt):
        """
        Args:
            model: MuJoCo model
            joint_ids: ì œì–´í•  ê´€ì ˆ ì¸ë±ìŠ¤
            horizon: MPC ì˜ˆì¸¡ êµ¬ê°„
            dt: ì‹œê°„ ê°„ê²©
        """
        self.model = model
        self.joint_ids = joint_ids
        self.nq = len(joint_ids)
        self.horizon = horizon
        self.dt = dt
        
        # MPC ë™ì—­í•™ ê³„ì‚°ìš© ë°ì´í„°
        self.data_temp = mujoco.MjData(model)
        
        # íŒŒë¼ë¯¸í„° ì´ˆê¸°ê°’ ì €ì¥
        self.theta_init = None
        
    def load_demonstration_data(self, data_path):
        """
        ì‹œì—° ë°ì´í„° ë¡œë“œ
        
        Args:
            data_path: .npz íŒŒì¼ ê²½ë¡œ
            
        Returns:
            demonstrations: List of dicts with keys:
                - 't': time
                - 'q': joint positions
                - 'qdot': joint velocities  
                - 'u': control inputs (tau)
                - 'q_ref': reference positions
        """
        data = np.load(data_path)
        
        # ì‹œì—° ë°ì´í„°ë¥¼ ì‹œê°„ êµ¬ê°„ìœ¼ë¡œ ë¶„í• 
        demonstrations = []
        
        # ì „ì²´ ë°ì´í„°ë¥¼ horizon ê¸¸ì´ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ëˆ”
        n_samples = len(data['q'])
        segment_length = self.horizon + 1
        
        for start_idx in range(0, n_samples - segment_length, segment_length // 2):
            end_idx = start_idx + segment_length
            
            demo = {
                'q': data['q'][start_idx:end_idx],
                'qdot': data['qdot'][start_idx:end_idx],
                'u': data['tau_mpc'][start_idx:end_idx],
                't': np.arange(segment_length) * self.dt
            }
            
            demonstrations.append(demo)
        
        print(f"âœ… Loaded {len(demonstrations)} demonstration segments")
        return demonstrations
    
    def compute_gradient_norm(self, theta, demonstration):
        """
        ì£¼ì–´ì§„ íŒŒë¼ë¯¸í„° Î¸ì— ëŒ€í•´ â€–âˆ‡Lâ€–Â² ê³„ì‚°
        
        ì´ê²ƒì´ ëª©ì í•¨ìˆ˜ì…ë‹ˆë‹¤. ì´ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” Î¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

        ì£¼ì–´ì§„ ë¹„ìš© ê°€ì¤‘ì¹˜ Î¸ê°€ ìˆì„ ë•Œ, ì „ë¬¸ê°€ ì‹œì—°ì´ ê·¸ ë¹„ìš©ì— ëŒ€í•´ â€˜ê±°ì˜ ìµœì â€™ì´ì—ˆëŠ”ì§€ë¥¼ KKT ì¡°ê±´ì˜ ì”ì°¨(âˆ‡L) í¬ê¸°ë¡œ í‰ê°€í•œë‹¤.
        
        Args:
            theta: ë¹„ìš©í•¨ìˆ˜ íŒŒë¼ë¯¸í„° [q_pos, q_vel, r_tau, q_terminal] (4ê°œ)
            demonstration: ì‹œì—° ë°ì´í„° dict
            
        Returns:
            gradient_norm_squared: â€–âˆ‡Lâ€–Â² ê°’
        """
        # íŒŒë¼ë¯¸í„° ì–¸íŒ©
        q_pos_weight = theta[0]
        q_vel_weight = theta[1] 
        r_tau_weight = theta[2]
        q_terminal_weight = theta[3]
        
        # ì‹œì—° ë°ì´í„° ì¶”ì¶œ
        q_demo = demonstration['q']      # (horizon+1, nq)
        qdot_demo = demonstration['qdot']
        u_demo = demonstration['u']      # (horizon+1, nq)
        
        # Lagrangian ê¸°ìš¸ê¸°ë¥¼ ì €ì¥í•  ë°°ì—´
        grad_L = np.zeros((self.horizon, self.nq))
        
        # ê° ì‹œê°„ ìŠ¤í…ì—ì„œ Lagrangianì˜ uì— ëŒ€í•œ í¸ë¯¸ë¶„ ê³„ì‚°
        for k in range(self.horizon):
            # í˜„ì¬ ìƒíƒœ
            q_k = q_demo[k]
            qdot_k = qdot_demo[k]
            u_k = u_demo[k]
            
            # ë‹¤ìŒ ìƒíƒœ (ì‹¤ì œ ì‹œì—°)
            q_next_demo = q_demo[k+1]
            qdot_next_demo = qdot_demo[k+1]
            
            # âˆ‚l/âˆ‚u ê³„ì‚° (stage costì˜ ì…ë ¥ì— ëŒ€í•œ ë¯¸ë¶„) -> ì–´ë–¤ ì…ë ¥ì´ ê°€ì¥ costë¥¼ ì‘ê²Œ í•˜ëŠ”ì§€ ê³„ì‚° 
            # l = q_pos * â€–q - q_refâ€–Â² + q_vel * â€–qdotâ€–Â² + r_tau * â€–uâ€–Â²
            dldu_direct = 2 * r_tau_weight * u_k
            
            # âˆ‚(next state)/âˆ‚u ê³„ì‚° (ë™ì—­í•™ ì œì•½ì˜ uì— ëŒ€í•œ ë¯¸ë¶„)
            # x_{k+1} = f(x_k, u_k)ì´ë¯€ë¡œ âˆ‚x_{k+1}/âˆ‚u_kë¥¼ êµ¬í•´ì•¼ í•¨
            
            # ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ ê·¼ì‚¬
            epsilon = 1e-6
            du = np.eye(self.nq) * epsilon
            
            # ë™ì—­í•™ ê³„ì‚°ì„ ìœ„í•œ ìƒíƒœ ì„¤ì •
            dynamics_grad = np.zeros((self.nq, self.nq))
            
            for i in range(self.nq):
                # u + epsilon
                u_plus = u_k.copy()
                u_plus[i] += epsilon
                q_next_plus, qdot_next_plus = self._forward_dynamics(q_k, qdot_k, u_plus)
                
                # u - epsilon  
                u_minus = u_k.copy()
                u_minus[i] -= epsilon
                q_next_minus, qdot_next_minus = self._forward_dynamics(q_k, qdot_k, u_minus)
                
                # ìˆ˜ì¹˜ ë¯¸ë¶„
                dq_next_du = (q_next_plus - q_next_minus) / (2 * epsilon)
                dqdot_next_du = (qdot_next_plus - qdot_next_minus) / (2 * epsilon)
                
                dynamics_grad[:, i] = dq_next_du
            
            # ë‹¤ìŒ ìŠ¤í…ì˜ ë¹„ìš©ì— ëŒ€í•œ ì˜í–¥ (ì²´ì¸ ë£°)
            # âˆ‚l_{k+1}/âˆ‚q_{k+1} * âˆ‚q_{k+1}/âˆ‚u_k
            if k < self.horizon - 1:
                # ì°¸ì¡°ê°’ (ì—¬ê¸°ì„œëŠ” ì‹œì—°ì˜ ë§ˆì§€ë§‰ ìƒíƒœë¥¼ ëª©í‘œë¡œ ê°€ì •)
                q_ref = q_demo[-1]
                q_error_next = q_demo[k+1] - q_ref
                
                dldu_chain = dynamics_grad.T @ (2 * q_pos_weight * q_error_next)
                dldu_chain += dynamics_grad.T @ (2 * q_vel_weight * qdot_demo[k+1])
            else:
                # ì¢…ë‹¨ ë¹„ìš©
                q_ref = q_demo[-1]
                q_error_terminal = q_demo[-1] - q_ref
                dldu_chain = dynamics_grad.T @ (2 * q_terminal_weight * q_error_terminal)
            
            # ì „ì²´ ê¸°ìš¸ê¸°
            grad_L[k] = dldu_direct + dldu_chain
        
        # â€–âˆ‡Lâ€–Â² ê³„ì‚°
        gradient_norm_squared = np.sum(grad_L ** 2)
        
        return gradient_norm_squared
    
    def _forward_dynamics(self, q, qdot, u):
        """
        í•œ ìŠ¤í… ë™ì—­í•™ ì‹œë®¬ë ˆì´ì…˜ (RK4)
        
        Args:
            q: ê´€ì ˆ ìœ„ì¹˜
            qdot: ê´€ì ˆ ì†ë„
            u: ì œì–´ ì…ë ¥
            
        Returns:
            q_next: ë‹¤ìŒ ìœ„ì¹˜
            qdot_next: ë‹¤ìŒ ì†ë„
        """
        # MuJoCo ìƒíƒœ ì„¤ì •
        self.data_temp.qpos[self.joint_ids] = q
        self.data_temp.qvel[self.joint_ids] = qdot
        
        # Forward dynamics
        mujoco.mj_forward(self.model, self.data_temp)
        
        # Mass matrix
        M_full = np.zeros((self.model.nv, self.model.nv))
        mujoco.mj_fullM(self.model, M_full, self.data_temp.qM)
        M = M_full[np.ix_(self.joint_ids, self.joint_ids)]
        
        # Bias force
        bias = self.data_temp.qfrc_bias[self.joint_ids]
        
        # ê°€ì†ë„ ê³„ì‚°
        qddot = np.linalg.solve(M, u - bias)
        
        # RK4 ì ë¶„
        k1_v = qdot
        k1_a = qddot
        
        k2_v = qdot + 0.5 * self.dt * k1_a
        k2_a = qddot  # ê°„ë‹¨í™”: ê°™ì€ ê°€ì†ë„ ì‚¬ìš©
        
        k3_v = qdot + 0.5 * self.dt * k2_a
        k3_a = qddot
        
        k4_v = qdot + self.dt * k3_a
        k4_a = qddot
        
        q_next = q + (self.dt / 6.0) * (k1_v + 2*k2_v + 2*k3_v + k4_v)
        qdot_next = qdot + (self.dt / 6.0) * (k1_a + 2*k2_a + 2*k3_a + 4*k4_a)
        
        return q_next, qdot_next
    
    def learn_cost_weights(self, demonstrations, theta_init=None):
        """
        ì‹œì—° ë°ì´í„°ë¡œë¶€í„° ë¹„ìš©í•¨ìˆ˜ ê°€ì¤‘ì¹˜ í•™ìŠµ
        
        Args:
            demonstrations: ì‹œì—° ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            theta_init: ì´ˆê¸° íŒŒë¼ë¯¸í„° (Noneì´ë©´ ìë™ ì„¤ì •)
            
        Returns:
            theta_opt: ìµœì  íŒŒë¼ë¯¸í„°
            result: ìµœì í™” ê²°ê³¼
        """
        # ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •
        if theta_init is None:
            # í•©ë¦¬ì ì¸ ì´ˆê¸°ê°’
            theta_init = np.array([
                1000.0,  # q_pos_weight
                50.0,    # q_vel_weight  
                0.01,    # r_tau_weight
                1500.0   # q_terminal_weight
            ])
        
        self.theta_init = theta_init.copy()
        
        print("\n" + "="*60)
        print("ğŸ¯ Inverse Optimal Control: Learning MPC Cost Weights")
        print("="*60)
        print(f"Method: Relaxed KKT (minimize â€–âˆ‡Lâ€–Â²)")
        print(f"Demonstrations: {len(demonstrations)} segments")
        print(f"Horizon: {self.horizon}")
        print(f"Initial parameters: {theta_init}")
        
        # ëª©ì í•¨ìˆ˜: ëª¨ë“  ì‹œì—°ì— ëŒ€í•œ í‰ê·  â€–âˆ‡Lâ€–Â²
        def objective(theta):
            total_grad_norm = 0.0
            for demo in demonstrations:
                grad_norm_sq = self.compute_gradient_norm(theta, demo)
                total_grad_norm += grad_norm_sq
            
            avg_grad_norm = total_grad_norm / len(demonstrations)
            return avg_grad_norm
        
        # ì œì•½ì¡°ê±´: ëª¨ë“  ê°€ì¤‘ì¹˜ëŠ” ì–‘ìˆ˜
        bounds = [(1e-3, None)] * len(theta_init)
        
        # ì •ê·œí™” ì œì•½: í•©ì´ ì¼ì • (ìŠ¤ì¼€ì¼ ê³ ì •)
        # ì´ê²ƒì´ ì—†ìœ¼ë©´ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´í•  ìˆ˜ ìˆìŒ
        total_weight = np.sum(theta_init)
        constraints = {
            'type': 'eq',
            'fun': lambda theta: np.sum(theta) - total_weight
        }
        
        print("\nâ–¶ï¸  Starting optimization...")
        print(f"   Bounds: all weights > 0")
        print(f"   Constraint: Î£Î¸ = {total_weight:.1f}")
        
        # ìµœì í™” ì‹¤í–‰
        result = minimize(
            objective,
            theta_init,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints,
            options={
                'maxiter': 100,
                'ftol': 1e-6,
                'disp': True
            }
        )
        
        theta_opt = result.x
        
        print("\nâœ… Optimization completed!")
        print(f"   Success: {result.success}")
        print(f"   Final â€–âˆ‡Lâ€–Â²: {result.fun:.6e}")
        print(f"   Iterations: {result.nit}")
        
        return theta_opt, result
    
    def compare_parameters(self, theta_learned, theta_original=None):
        """
        í•™ìŠµëœ íŒŒë¼ë¯¸í„°ì™€ ì›ë˜ íŒŒë¼ë¯¸í„° ë¹„êµ
        
        Args:
            theta_learned: í•™ìŠµëœ íŒŒë¼ë¯¸í„°
            theta_original: ì›ë˜ íŒŒë¼ë¯¸í„° (Noneì´ë©´ ì´ˆê¸°ê°’ ì‚¬ìš©)
        """
        if theta_original is None:
            theta_original = self.theta_init
        
        param_names = ['Q_pos', 'Q_vel', 'R_tau', 'Q_terminal']
        
        print("\n" + "="*60)
        print("ğŸ“Š Parameter Comparison")
        print("="*60)
        print(f"{'Parameter':<15} {'Original':>12} {'Learned':>12} {'Ratio':>10}")
        print("-"*60)
        
        for i, name in enumerate(param_names):
            orig = theta_original[i]
            learned = theta_learned[i]
            ratio = learned / orig if orig != 0 else float('inf')
            
            print(f"{name:<15} {orig:>12.2f} {learned:>12.2f} {ratio:>10.2f}x")
        
        print("="*60)
    
    def visualize_results(self, theta_learned, demonstrations, n_samples=3):
        """
        í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
        
        Args:
            theta_learned: í•™ìŠµëœ íŒŒë¼ë¯¸í„°
            demonstrations: ì‹œì—° ë°ì´í„°
            n_samples: ì‹œê°í™”í•  ì‹œì—° ê°œìˆ˜
        """
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # ìƒ˜í”Œ ì„ íƒ
        demo_indices = np.linspace(0, len(demonstrations)-1, n_samples, dtype=int)
        
        for demo_idx in demo_indices:
            demo = demonstrations[demo_idx]
            
            # ê¸°ìš¸ê¸° ê³„ì‚°
            grad_norm_init = self.compute_gradient_norm(self.theta_init, demo)
            grad_norm_learned = self.compute_gradient_norm(theta_learned, demo)
            
            # ì‹œì—° ë°ì´í„°
            t = demo['t']
            q = demo['q'][:, 0]  # ì²« ë²ˆì§¸ ê´€ì ˆë§Œ
            qdot = demo['qdot'][:, 0]
            u = demo['u'][:, 0]
            
            # Plot 1: Position trajectory
            axes[0, 0].plot(t, q, alpha=0.6, label=f'Demo {demo_idx}')
            
            # Plot 2: Velocity  
            axes[0, 1].plot(t, qdot, alpha=0.6)
            
            # Plot 3: Control input
            axes[1, 0].plot(t, u, alpha=0.6)
        
        # Plot 4: Gradient norm comparison
        grad_norms_init = []
        grad_norms_learned = []
        
        for demo in demonstrations:
            grad_norms_init.append(
                np.sqrt(self.compute_gradient_norm(self.theta_init, demo))
            )
            grad_norms_learned.append(
                np.sqrt(self.compute_gradient_norm(theta_learned, demo))
            )
        
        x = np.arange(len(demonstrations))
        axes[1, 1].bar(x - 0.2, grad_norms_init, 0.4, 
                      label='Initial Î¸', alpha=0.7)
        axes[1, 1].bar(x + 0.2, grad_norms_learned, 0.4,
                      label='Learned Î¸', alpha=0.7)
        
        # ë ˆì´ë¸” ì„¤ì •
        axes[0, 0].set_ylabel('Position [rad]')
        axes[0, 0].set_title('Demonstration Trajectories - Position')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        axes[0, 1].set_ylabel('Velocity [rad/s]')
        axes[0, 1].set_title('Demonstration Trajectories - Velocity')
        axes[0, 1].grid(True, alpha=0.3)
        
        axes[1, 0].set_xlabel('Time [s]')
        axes[1, 0].set_ylabel('Torque [Nm]')
        axes[1, 0].set_title('Demonstration Trajectories - Control')
        axes[1, 0].grid(True, alpha=0.3)
        
        axes[1, 1].set_xlabel('Demonstration Index')
        axes[1, 1].set_ylabel('â€–âˆ‡Lâ€–')
        axes[1, 1].set_title('KKT Gradient Norm Comparison')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('/mnt/user-data/outputs/ioc_results.png', dpi=150)
        print("\nâœ… Visualization saved: ioc_results.png")
        plt.show()


def apply_learned_weights_to_mpc(mpc_controller, theta_learned):
    """
    í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ MPC ì»¨íŠ¸ë¡¤ëŸ¬ì— ì ìš©
    
    Args:
        mpc_controller: TorqueMPC ì¸ìŠ¤í„´ìŠ¤
        theta_learned: í•™ìŠµëœ íŒŒë¼ë¯¸í„° [q_pos, q_vel, r_tau, q_terminal]
    """
    nq = mpc_controller.nq
    
    Q_pos = np.eye(nq) * theta_learned[0]
    Q_vel = np.eye(nq) * theta_learned[1]
    R_tau = np.eye(nq) * theta_learned[2]
    Q_terminal = np.eye(nq) * theta_learned[3]
    
    mpc_controller.update_cost_weights(
        Q_pos=Q_pos,
        Q_vel_ref=Q_vel,
        R_tau=R_tau,
        Q_terminal=Q_terminal
    )
    
    print("\nâœ… Learned weights applied to MPC controller!")
    print(f"   Q_pos: {theta_learned[0]:.2f}")
    print(f"   Q_vel: {theta_learned[1]:.2f}")
    print(f"   R_tau: {theta_learned[2]:.2f}")
    print(f"   Q_terminal: {theta_learned[3]:.2f}")